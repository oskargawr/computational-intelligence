1) Standard scaler to klasa sluzaca do standaryzacji cech poprzez usuniecie sredniej (centrowanie
danych na zero) i skalowanie do jednostkowej wariancji. Dane liczbowe sa transformowane wedlug
nastepujacego wzoru: z = (x - u) / s, gdzie x to wartosc danej cechy, u to srednia cecha, a s to
odchylenie standardowe danej cechy.

2) Sluzy do przeksztalcania kategorycznych lub dyskretnych danych wejsciowych w format, ktory moze byc
lepiej zrozumiany przez algorytmy uczenia maszynowego. Kodowanie "one hot" znane rowniez jako 
"one-of-K" lub "dummy", polega na przeksztalceniu kazdej kategorii w danej cechy na osobna ceche
binarna (0 lub 1). Przykladowo, dla danych wejsciowych
kolor
-----
czerwony
zielony
niebieski

zwroci to:
jest_czerwony  jest_zielony  jest_niebieski
-------------  ------------  --------------
1              0             0
0              1             0
0              0             1
1              0             0

3) X_train.shape[1] i y_encoded.shape[1] sa uzywane do okreslania liczby neuronow w warstwach
wejsciowej i wyjsciowej modelu.

X_train.shape[1] oznacza liczbe kolumn w X_train, czyli liczbe cech wejsciowych. Kazda cecha wejsciowa
jest reprezentowana przez jeden neuron w warstwie wejsciowej, wiec liczba neuronow w warstwie wejsciowej
jest rowna liczbie cech wejsciowych

4) relu - 100%
tanh - 97.78%
sigmoid - 95.56%

w tym przypadku relu okazalo sie byc najskuteczniejsze

5) tak, zmiana optymalizatora, funkcji straty i innych hiperparametrow moze wplywac na wyniki modelu. przetestowalem kod dla optimizier=SGD z learning rate 0.01 i dostalem wynik 86.67%

6) tak, mozna zmienic rozmiar partii dodajac parametr batch_size do funkcji fit. Przykladowo, batch_size=32 oznacza, ze model bedzie aktualizowany po kazdych 32 przykladach treningowych. dalem batch_size 8 dla funkcji relu, wynik pozostal 100% ale krzywe uczenia i straty sie zmienily

7) do ok. 50 epoki model uczy sie w sposob zrownowazony i dobrze generalizuje dane. Model nie jest przeuczony ani niedouczony. pod koniec model zaczal sie zblizac do optymalnego dopasowania do danych treningowych. Pod koniec validation loss zaczyna falowac, co moze oznaczac, ze model zaczyna sie przeuczac.

8) w ponizszym kodzie na poczatku sa ladowane niezbedne biblioteki, zaladowanie danych, przetwarzanie danych (skalowanie cech za pomoca StandardScaler, kodowanie etykiet za pomoca OneHotEncoder), podzial danych na zestawy treningowe i testowe, zaladowanie wczesniej wytrenowanego modelu iris_model.h6, trenowanie modelu przez dodatkowe 10 epok, zapisanie zaaktualizowanego modelu do pliku updated_iris_model.h5, a na koncu ewaluacja modelu.